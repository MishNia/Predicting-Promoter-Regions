{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjVL09cMJ11Z",
        "outputId": "00fb0ec6-a586-4510-ce0a-2bf2cf1aae1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V1uqXcMXJsAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import math\n",
        "import argparse\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "G2srD2gLLDhY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filepath):\n",
        "    # Open and read lines from a file\n",
        "    with open(filepath, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    # Remove extra whitespace from each line\n",
        "    lines = [line.strip() for line in lines]\n",
        "    return lines"
      ],
      "metadata": {
        "id": "SAGQYw81p0RW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_kmer_list(kmer_length, alphabet=\"TGCA\"):\n",
        "    # Convert the alphabet string to a list of characters\n",
        "    characters = list(alphabet)\n",
        "    # Create all possible k-mers of the specified length\n",
        "    kmers = [\"\".join(combination) for combination in itertools.product(characters, repeat=kmer_length)]\n",
        "    # Sort the k-mers alphabetically\n",
        "    kmers.sort()\n",
        "    return kmers"
      ],
      "metadata": {
        "id": "dzb6pBxJp5WO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def protein_to_numeric(protein_sequence, kmer_list):\n",
        "    # Get the k-mer length from the first element of the kmer list\n",
        "    kmer_length = len(kmer_list[0])\n",
        "    # Create a dictionary mapping k-mers to their indices\n",
        "    kmer_dict = {kmer: index for index, kmer in enumerate(kmer_list)}\n",
        "    numeric_representation = []\n",
        "\n",
        "    # Convert each k-mer in the protein sequence to its numeric index\n",
        "    for i in range(len(protein_sequence) - kmer_length + 1):\n",
        "        try:\n",
        "            numeric_representation.append(kmer_dict[protein_sequence[i:i+kmer_length]])\n",
        "        except KeyError:\n",
        "            # Handle k-mers not found in the dictionary\n",
        "            numeric_representation.append(0)\n",
        "            print(\"Key error:\", protein_sequence[i:i+kmer_length], \"at index\", i)\n",
        "\n",
        "    return numeric_representation"
      ],
      "metadata": {
        "id": "bBGvmQYrp8Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_negative_samples(protein_sequence, num_segments=20, segments_to_keep=8, max_value=4):\n",
        "    sequence_length = len(protein_sequence)\n",
        "    # Calculate the length of each segment\n",
        "    segment_length = sequence_length // num_segments\n",
        "\n",
        "    # Adjust the number of segments if there's a remainder\n",
        "    if segment_length * num_segments < sequence_length:\n",
        "        num_segments += 1\n",
        "\n",
        "    # Generate an array of segment indices\n",
        "    segment_indices = np.arange(num_segments)\n",
        "    # Randomly select segments to keep\n",
        "    retained_segments = random.sample(list(segment_indices), k=segments_to_keep)\n",
        "    output_sequence = []\n",
        "\n",
        "    # Iterate over each segment\n",
        "    for segment_index in segment_indices:\n",
        "        start_index = segment_index * segment_length\n",
        "        segment = protein_sequence[start_index:start_index + segment_length]\n",
        "        if segment_index in retained_segments:\n",
        "            # Keep the original segment\n",
        "            output_sequence.extend(segment)\n",
        "        else:\n",
        "            # Replace the segment with random values\n",
        "            random_segment = random.choices(np.arange(max_value), k=len(segment))\n",
        "            output_sequence.extend(random_segment)\n",
        "\n",
        "    return output_sequence"
      ],
      "metadata": {
        "id": "SlV6H1vtp_a1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotDataset(Dataset):\n",
        "    def __init__(self, file_path, is_positive=True, device=\"cuda\", fake_type=0, sequence_length=300, segments=20, keep_segments=8):\n",
        "        \"\"\"\n",
        "        Initializes the dataset for one-hot encoding of protein sequences.\n",
        "\n",
        "        :param file_path: Path to the text file containing the dataset.\n",
        "        :param is_positive: Indicates the dataset label; True for positive (1), False for negative (0).\n",
        "        :param device: Specifies the device to use (e.g., \"cuda\" or \"cpu\").\n",
        "        :param fake_type: Defines the type of fake data generation;\n",
        "                          0 for loading the original dataset,\n",
        "                          1 for generating randomized sequences,\n",
        "                          2 for generating fake sequences using a specified method.\n",
        "        :param sequence_length: Length of the input protein sequence.\n",
        "        :param segments: Number of segments to divide the sequence into for fake data generation.\n",
        "        :param keep_segments: Number of segments to retain unchanged during fake sequence generation.\n",
        "        \"\"\"\n",
        "        if is_positive and fake_type != 0:\n",
        "            raise ValueError(\"The 'fake_type' parameter cannot be used with a positive dataset.\")\n",
        "\n",
        "        self.device = device\n",
        "        self.fake_type = fake_type\n",
        "        self.sequence_length = sequence_length\n",
        "        self.segments = segments\n",
        "        self.keep_segments = keep_segments\n",
        "\n",
        "        # Generate the k-mer dictionary\n",
        "        kmer_dict = generate_kmer_list(1)\n",
        "\n",
        "        # Load data from the specified file\n",
        "        self.protein_data = read_file(file_path)\n",
        "\n",
        "        # Convert protein sequences into numeric representations\n",
        "        self.numeric_data = [protein_to_numeric(protein, kmer_dict) for protein in self.protein_data]\n",
        "\n",
        "        # Assign labels based on whether the dataset is positive or negative\n",
        "        if is_positive:\n",
        "            self.labels = torch.from_numpy(np.ones(len(self.protein_data)))\n",
        "        else:\n",
        "            self.labels = torch.from_numpy(np.zeros(len(self.protein_data)))\n",
        "        self.labels = self.labels.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples in the dataset\n",
        "        return len(self.protein_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve and pad/crop the numeric sequence to the fixed length\n",
        "        sequence = self.numeric_data[index]\n",
        "        if len(sequence) < self.sequence_length:\n",
        "            sequence.extend([0] * (self.sequence_length - len(sequence)))\n",
        "        elif len(sequence) > self.sequence_length:\n",
        "            sequence = sequence[:self.sequence_length]\n",
        "\n",
        "        # Generate fake data if specified\n",
        "        if self.fake_type == 1:\n",
        "            random.shuffle(sequence)  # Shuffle the sequence\n",
        "        elif self.fake_type == 2:\n",
        "            sequence = generate_negative_samples(sequence, num_part=self.segments, keep=self.keep_segments, max_class=4)\n",
        "\n",
        "        # Convert the sequence to a tensor and one-hot encode it\n",
        "        sequence_tensor = torch.from_numpy(np.array(sequence))\n",
        "        one_hot_encoded = torch.nn.functional.one_hot(sequence_tensor, num_classes=4).to(self.device)\n",
        "\n",
        "        return one_hot_encoded.float(), self.labels[index]"
      ],
      "metadata": {
        "id": "mtsXPn1LKoLR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path, train_ratio=0.8, add_random_neg=False, batch_size=32, num_workers=0, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load and prepare datasets for training, validation, and testing.\n",
        "\n",
        "    :param data_path: Path to the text file containing promoter sequences (one DNA promoter per line).\n",
        "    :param train_ratio: Proportion of the dataset allocated for training.\n",
        "    :param add_random_neg: Whether to include random DNA sequences in the negative dataset.\n",
        "    :param batch_size: Number of samples per batch for the data loader.\n",
        "    :param num_workers: Number of CPU threads for loading data in parallel.\n",
        "    :param device: Device on which data should be loaded (e.g., \"cuda\" or \"cpu\").\n",
        "    :return: A list of data loaders for training, validation, and testing for positive and negative datasets.\n",
        "    \"\"\"\n",
        "    # Set a manual seed for reproducibility\n",
        "    seed_generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "    # Initialize positive and negative datasets\n",
        "    positive_dataset = OneHotDataset(data_path, device=device)\n",
        "    negative_dataset = OneHotDataset(data_path, is_pos=False, fake=2, device=device)\n",
        "\n",
        "    # Calculate the sizes for training, validation, and testing splits\n",
        "    train_size = int(len(positive_dataset) * train_ratio)\n",
        "    val_size = int(len(positive_dataset) * (1 - train_ratio) * 0.5)\n",
        "    split_sizes = [train_size, val_size, len(positive_dataset) - train_size - val_size]\n",
        "\n",
        "    # Split the datasets into training, validation, and testing subsets\n",
        "    train_positive, val_positive, test_positive = random_split(positive_dataset, split_sizes, generator=seed_generator)\n",
        "    train_negative, val_negative, test_negative = random_split(negative_dataset, split_sizes, generator=seed_generator)\n",
        "\n",
        "    # Optionally, add a random negative dataset to the training subset\n",
        "    if add_random_neg:\n",
        "        random_negative_dataset = OneHotDataset(data_path, is_pos=False, fake=1, device=device)\n",
        "        train_negative = ConcatDataset([train_negative, random_negative_dataset])\n",
        "\n",
        "    # Prepare data loaders for each dataset split\n",
        "    datasets = [train_positive, val_positive, test_positive, train_negative, val_negative, test_negative]\n",
        "    data_loaders = [\n",
        "        DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        for dataset in datasets\n",
        "    ]\n",
        "\n",
        "    return data_loaders"
      ],
      "metadata": {
        "id": "9GLVqpohKrYu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_data(data_path, batch_size=32, device=\"cuda\", num_workers=0):\n",
        "    \"\"\"\n",
        "    Load the test dataset and prepare a data loader.\n",
        "\n",
        "    :param data_path: Path to the text file containing test data (one DNA sequence per line).\n",
        "    :param batch_size: Number of samples per batch for the data loader.\n",
        "    :param device: Device on which the data will be loaded (e.g., \"cuda\" or \"cpu\").\n",
        "    :param num_workers: Number of CPU threads for parallel data loading.\n",
        "    :return: A DataLoader for the test dataset.\n",
        "    \"\"\"\n",
        "    # Create a dataset for one-hot encoded DNA sequences\n",
        "    test_dataset = OneHotDataset(data_path, device=device)\n",
        "\n",
        "    # Initialize a data loader without shuffling for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return test_loader\n"
      ],
      "metadata": {
        "id": "XX32IszfKtya"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelCNN(nn.Module):\n",
        "    def __init__(self, kernel_sizes, pooling_size=6, dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        A module that applies multiple CNN layers in parallel to the input\n",
        "        and concatenates their outputs.\n",
        "\n",
        "        :param kernel_sizes: A list of kernel sizes for the convolutional layers.\n",
        "        :param pooling_size: Size of the pooling layer applied after each CNN.\n",
        "        :param dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(ParallelCNN, self).__init__()\n",
        "        self.cnn_layers = nn.ModuleList()\n",
        "        for kernel_size in kernel_sizes:\n",
        "            # Define a sequential model for each kernel size\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv1d(in_channels=4, out_channels=4, kernel_size=kernel_size, padding=\"same\"),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(kernel_size=pooling_size),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.cnn_layers.append(layer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the parallel CNN layers.\n",
        "\n",
        "        :param inputs: Input tensor representing one-hot encoded DNA sequences\n",
        "                       with shape [batch_size, 4, sequence_length].\n",
        "        :return: Concatenated output from all CNN layers\n",
        "                 with shape [batch_size, 4 * len(kernel_sizes), reduced_length].\n",
        "        \"\"\"\n",
        "        outputs = []\n",
        "        for layer in self.cnn_layers:\n",
        "            outputs.append(layer(inputs))\n",
        "        # Concatenate outputs from all convolutional layers along the channel dimension\n",
        "        concatenated_output = torch.cat(outputs, dim=1)\n",
        "        return concatenated_output"
      ],
      "metadata": {
        "id": "6jOaIk2HLLbF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        A Bidirectional LSTM followed by a fully connected layer.\n",
        "\n",
        "        :param input_dim: Size of the input features.\n",
        "        :param hidden_dim: Number of hidden units in the LSTM.\n",
        "        :param output_dim: Size of the output features.\n",
        "        \"\"\"\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        # Define a bidirectional LSTM\n",
        "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "        # Define a linear layer to transform LSTM output to desired output size\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the Bidirectional LSTM.\n",
        "\n",
        "        :param inputs: Input tensor with shape [batch_size, sequence_length, input_dim].\n",
        "        :return: Output tensor with shape [batch_size, sequence_length, output_dim].\n",
        "        \"\"\"\n",
        "        # Optimize for better performance on multi-GPU setups\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "        # Pass the input through the LSTM\n",
        "        lstm_output, _ = self.rnn(inputs)  # Shape: [batch_size, sequence_length, 2*hidden_dim]\n",
        "\n",
        "        # Transform the LSTM output with the linear layer\n",
        "        output = self.fc(lstm_output)  # Shape: [batch_size, sequence_length, output_dim]\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZpMjSRUhLRHa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeePromoter(nn.Module):\n",
        "    def __init__(self, kernel_sizes, input_shape=(64, 300, 4), pooling_size=6, dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        DeePromoter Model: Combines parallel CNNs, a Bidirectional LSTM,\n",
        "        and fully connected layers for sequence classification.\n",
        "\n",
        "        :param kernel_sizes: A list of kernel sizes for the parallel CNN layers.\n",
        "        :param input_shape: Fixed input shape for the model (batch_size, sequence_length, num_channels).\n",
        "        :param pooling_size: Pooling kernel size for the CNN layers.\n",
        "        :param dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(DeePromoter, self).__init__()\n",
        "\n",
        "        # Number of output channels from the ParallelCNN\n",
        "        lstm_input_dim = len(kernel_sizes) * 4\n",
        "\n",
        "        # Define the model components\n",
        "        self.parallel_cnn = ParallelCNN(kernel_sizes, pooling_size, dropout_rate)\n",
        "        self.bidirectional_lstm = BidirectionalLSTM(input_size=lstm_input_dim,\n",
        "                                                    hidden_size=lstm_input_dim,\n",
        "                                                    output_size=lstm_input_dim)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Determine the flattened feature size using a dummy input\n",
        "        dummy_input = torch.zeros(input_shape)\n",
        "        flattened_size = self.compute_flattened_size(dummy_input)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(flattened_size, flattened_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(flattened_size, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def compute_flattened_size(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the feature size after the convolutional and LSTM layers,\n",
        "        before passing through the fully connected layers.\n",
        "\n",
        "        :param x: Dummy input tensor with shape [batch_size, sequence_length, num_channels].\n",
        "        :return: Size of the flattened features.\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)  # Adjust dimensions for CNN input\n",
        "        x = self.parallel_cnn(x)\n",
        "        x = x.permute(0, 2, 1)  # Adjust dimensions for LSTM input\n",
        "        x = self.bidirectional_lstm(x)\n",
        "        x = self.flatten(x)\n",
        "        return x.shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the DeePromoter model.\n",
        "\n",
        "        :param x: Input tensor with shape [batch_size, sequence_length, num_channels].\n",
        "        :return: Output tensor with shape [batch_size, 2], representing class probabilities.\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)  # Adjust dimensions for CNN input\n",
        "        x = self.parallel_cnn(x)\n",
        "        x = x.permute(0, 2, 1)  # Adjust dimensions for LSTM input\n",
        "        x = self.bidirectional_lstm(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YiJpX9CzMSog"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loaders):\n",
        "    \"\"\"\n",
        "    Perform inference and evaluate the model's predictions against the true labels.\n",
        "\n",
        "    :param model: The trained model set to evaluation mode.\n",
        "    :param data_loaders: A list of PyTorch DataLoader objects for evaluation.\n",
        "    :return: A tuple containing:\n",
        "             - A list of [correct_predictions, total_samples] for each DataLoader.\n",
        "             - A list of predicted results as integers for each DataLoader.\n",
        "    \"\"\"\n",
        "    evaluation_results = []\n",
        "    total_samples_list = []\n",
        "    correct_predictions_list = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for loader in data_loaders:\n",
        "        total_samples = 0\n",
        "        correct_predictions = 0\n",
        "        loader_predictions = []\n",
        "\n",
        "        for batch in loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "\n",
        "            # Update total sample count and correct predictions\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store the predictions\n",
        "            loader_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate accuracy for this DataLoader\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        evaluation_results.append(accuracy)\n",
        "        correct_predictions_list.append(correct_predictions)\n",
        "        total_samples_list.append(total_samples)\n",
        "        all_predictions.append(loader_predictions)\n",
        "\n",
        "    return (correct_predictions_list, total_samples_list), all_predictions"
      ],
      "metadata": {
        "id": "fpmiZtY_MZTv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcc(data):\n",
        "    \"\"\"\n",
        "    Calculate the Matthews Correlation Coefficient (MCC), along with precision and recall.\n",
        "\n",
        "    :param data: A list containing evaluation results, where:\n",
        "                 - `data[0]` represents the counts for positive results [True Positive, True Negative].\n",
        "                 - `data[1]` represents the total counts [Total Positive, Total Negative].\n",
        "    :return: A tuple of (precision, recall, MCC).\n",
        "    \"\"\"\n",
        "    # Extract true positive and true negative counts\n",
        "    true_positive = data[0][0]\n",
        "    true_negative = data[0][1]\n",
        "\n",
        "    # Extract total positive and negative counts\n",
        "    total_positive = data[1][0]\n",
        "    total_negative = data[1][1]\n",
        "\n",
        "    # Calculate false negatives and false positives\n",
        "    false_negative = total_positive - true_positive\n",
        "    false_positive = total_negative - true_negative\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = true_positive / (true_positive + false_positive)\n",
        "    recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "    # Calculate Matthews Correlation Coefficient\n",
        "    numerator = (true_positive * true_negative) - (false_positive * false_negative)\n",
        "    denominator = math.sqrt(\n",
        "        (true_positive + false_positive) *\n",
        "        (true_positive + false_negative) *\n",
        "        (true_negative + false_positive) *\n",
        "        (true_negative + false_negative)\n",
        "    )\n",
        "    mcc = numerator / denominator\n",
        "\n",
        "    return precision, recall, mcc"
      ],
      "metadata": {
        "id": "mTNEUWx4MfTF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(data_path, pretrained_model_path, kernel_sizes=None):\n",
        "    \"\"\"\n",
        "    Evaluate a pretrained DeePromoter model on a test dataset.\n",
        "\n",
        "    :param data_path: Path to the test dataset file.\n",
        "    :param pretrained_model_path: Path to the pretrained model file (state dictionary).\n",
        "    :param kernel_sizes: List of kernel sizes for the CNN layers in the DeePromoter model.\n",
        "                         Defaults to [27, 14, 7].\n",
        "    :return: A list of predicted results from the test dataset.\n",
        "    \"\"\"\n",
        "    if kernel_sizes is None:\n",
        "        kernel_sizes = [27, 14, 7]\n",
        "\n",
        "    # Load the test data\n",
        "    test_loader = load_test_data(data_path, device=device)\n",
        "\n",
        "    # Initialize the DeePromoter model\n",
        "    model = DeePromoter(kernel_sizes)\n",
        "    model.to(device)\n",
        "\n",
        "    # Load the pretrained model weights\n",
        "    model.load_state_dict(torch.load(pretrained_model_path))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the test dataset\n",
        "    evaluation_data, predictions = evaluate(model, [test_loader])\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "IAaVDmSFMiIp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_main():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "    \"-d\",\n",
        "    \"--data\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"path to dataset(txt file)\",\n",
        "  )\n",
        "  parser.add_argument(\"-w\", \"--weight\", type=str, help=\"Path to pre-train\")\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  output = test(args.data, args.weight)\n",
        "\n",
        "  with open(\"infer_results.txt\", \"w\") as f:\n",
        "    for out in output[0]:\n",
        "      f.write(str(out) + \"\\n\")"
      ],
      "metadata": {
        "id": "rjX_KyVCMlFO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "zwZMjEgnNjYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_path, pretrained_weights=None, experiment_name=\"test\", is_training=True, kernel_sizes=None, max_epochs=1000):\n",
        "    \"\"\"\n",
        "    Train or evaluate the DeePromoter model.\n",
        "\n",
        "    :param data_path: Path to the text file containing training data.\n",
        "    :param pretrained_weights: Path to pretrained model weights for continued training.\n",
        "    :param experiment_name: Name of the folder where results will be saved.\n",
        "    :param is_training: If False, performs testing only.\n",
        "    :param kernel_sizes: List of kernel sizes for the CNN layers in the DeePromoter model.\n",
        "    :param max_epochs: Maximum number of epochs for training.\n",
        "    \"\"\"\n",
        "    if kernel_sizes is None:\n",
        "        kernel_sizes = [27, 14, 7]\n",
        "\n",
        "    # Create output directories\n",
        "    output_dir = Path(\"./output\")\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    experiment_dir = output_dir.joinpath(experiment_name)\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    data_loaders = load_data(data_path, device=device)\n",
        "    train_pos, val_pos, test_pos, train_neg, val_neg, test_neg = data_loaders\n",
        "\n",
        "    # Initialize the model\n",
        "    model = DeePromoter(kernel_sizes)\n",
        "    model.to(device)\n",
        "\n",
        "    # Load pretrained weights if provided\n",
        "    if pretrained_weights is not None:\n",
        "        model.load_state_dict(torch.load(pretrained_weights))\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    # Training variables\n",
        "    running_loss = 0.0\n",
        "    best_mcc = 0\n",
        "    best_precision = 0\n",
        "    best_recall = 0\n",
        "    early_stop_threshold = 10\n",
        "    no_improvement_epochs = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    if is_training:\n",
        "        for epoch in range(max_epochs):\n",
        "            for (batch_pos, batch_neg) in zip(train_pos, train_neg):\n",
        "                # Prepare inputs and labels\n",
        "                inputs = torch.cat((batch_pos[0], batch_neg[0]), dim=0)\n",
        "                labels = torch.cat((batch_pos[1], batch_neg[1]), dim=0)\n",
        "\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass and compute loss\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Save and evaluate every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                torch.save(model.state_dict(), str(experiment_dir.joinpath(f\"epoch_{epoch}.pth\")))\n",
        "                model.eval()\n",
        "                eval_data, _ = evaluate(model, [val_pos, val_neg])\n",
        "                precision, recall, mcc_value = mcc(eval_data)\n",
        "                model.train()\n",
        "\n",
        "                print(f\"Epoch: {epoch}, Experiment: {experiment_name}\")\n",
        "                print(f\"Precision: {precision}\")\n",
        "                print(f\"Recall: {recall}\")\n",
        "                print(f\"MCC: {mcc_value}\")\n",
        "\n",
        "                # Save the best model based on precision, recall, and MCC\n",
        "                if precision > best_precision:\n",
        "                    best_precision = precision\n",
        "                    print(\"Updating best precision model...\")\n",
        "                    torch.save(model.state_dict(), str(experiment_dir.joinpath(\"best_precision.pth\")))\n",
        "                if recall > best_recall:\n",
        "                    best_recall = recall\n",
        "                    print(\"Updating best recall model...\")\n",
        "                    torch.save(model.state_dict(), str(experiment_dir.joinpath(\"best_recall.pth\")))\n",
        "                if mcc_value > best_mcc:\n",
        "                    best_mcc = mcc_value\n",
        "                    print(\"Updating best MCC model...\")\n",
        "                    torch.save(model.state_dict(), str(experiment_dir.joinpath(\"best_mcc.pth\")))\n",
        "                    no_improvement_epochs = 0\n",
        "                else:\n",
        "                    no_improvement_epochs += 1\n",
        "\n",
        "                # Stop training early if no improvement in MCC\n",
        "                if no_improvement_epochs >= early_stop_threshold:\n",
        "                    break\n",
        "\n",
        "        # Testing the model with the best MCC\n",
        "        best_model_path = experiment_dir.joinpath(\"best_mcc.pth\")\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        model.eval()\n",
        "        eval_data, _ = evaluate(model, [test_pos, test_neg])\n",
        "        precision, recall, mcc_value = mcc(eval_data)\n",
        "        print(\"Test Results:\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"MCC: {mcc_value}\")\n",
        "\n",
        "        # Save test results to a log file\n",
        "        with open(str(experiment_dir.joinpath(\"log.txt\")), \"w\") as log_file:\n",
        "            log_file.write(f\"Test Precision: {precision}\\n\")\n",
        "            log_file.write(f\"Test Recall: {recall}\\n\")\n",
        "            log_file.write(f\"Test MCC: {mcc_value}\\n\")"
      ],
      "metadata": {
        "id": "lV9rSDiSNg3j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('/content/drive/MyDrive/data/human/TATA/hs_pos_TATA.txt', exp_name=\"human_TATA\", training=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89B37VmOPGro",
        "outputId": "fa42abf2-e9e1-472d-ea9e-03b2e1c7de40"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
            "  return F.conv1d(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training\n",
            "Epoch : 0 Experiment : human_TATA\n",
            "precision : 0.7281553398058253\n",
            "recall : 0.2568493150684932\n",
            "MCC : 0.2111575952326206\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 10 Experiment : human_TATA\n",
            "precision : 0.7286432160804021\n",
            "recall : 0.4965753424657534\n",
            "MCC : 0.3287641768278304\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 20 Experiment : human_TATA\n",
            "precision : 0.79375\n",
            "recall : 0.4349315068493151\n",
            "MCC : 0.3608983811399463\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 30 Experiment : human_TATA\n",
            "precision : 0.8275862068965517\n",
            "recall : 0.410958904109589\n",
            "MCC : 0.3765367277077475\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 40 Experiment : human_TATA\n",
            "precision : 0.8266666666666667\n",
            "recall : 0.4246575342465753\n",
            "MCC : 0.38409228281811403\n",
            "Update best MCC\n",
            "Epoch : 50 Experiment : human_TATA\n",
            "precision : 0.8641975308641975\n",
            "recall : 0.4794520547945205\n",
            "MCC : 0.4513030562052148\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 60 Experiment : human_TATA\n",
            "precision : 0.8789473684210526\n",
            "recall : 0.571917808219178\n",
            "MCC : 0.5263051027501737\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 70 Experiment : human_TATA\n",
            "precision : 0.8647342995169082\n",
            "recall : 0.613013698630137\n",
            "MCC : 0.5405316138836923\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 80 Experiment : human_TATA\n",
            "precision : 0.8564593301435407\n",
            "recall : 0.613013698630137\n",
            "MCC : 0.5322277216449743\n",
            "Epoch : 90 Experiment : human_TATA\n",
            "precision : 0.8537735849056604\n",
            "recall : 0.6198630136986302\n",
            "MCC : 0.5341360109089501\n",
            "Update best recall\n",
            "Epoch : 100 Experiment : human_TATA\n",
            "precision : 0.8611111111111112\n",
            "recall : 0.636986301369863\n",
            "MCC : 0.5533167449931865\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 110 Experiment : human_TATA\n",
            "precision : 0.8726415094339622\n",
            "recall : 0.6335616438356164\n",
            "MCC : 0.5626232648240941\n",
            "Update best MCC\n",
            "Epoch : 120 Experiment : human_TATA\n",
            "precision : 0.9018691588785047\n",
            "recall : 0.660958904109589\n",
            "MCC : 0.6112525701138818\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 130 Experiment : human_TATA\n",
            "precision : 0.8508771929824561\n",
            "recall : 0.6643835616438356\n",
            "MCC : 0.5616005961955094\n",
            "Update best recall\n",
            "Epoch : 140 Experiment : human_TATA\n",
            "precision : 0.8755555555555555\n",
            "recall : 0.6746575342465754\n",
            "MCC : 0.594631923738699\n",
            "Update best recall\n",
            "Epoch : 150 Experiment : human_TATA\n",
            "precision : 0.953125\n",
            "recall : 0.6267123287671232\n",
            "MCC : 0.6342428798277872\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 160 Experiment : human_TATA\n",
            "precision : 0.9252336448598131\n",
            "recall : 0.678082191780822\n",
            "MCC : 0.6467905102367819\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 170 Experiment : human_TATA\n",
            "precision : 0.9045454545454545\n",
            "recall : 0.6815068493150684\n",
            "MCC : 0.6290106085387953\n",
            "Update best recall\n",
            "Epoch : 180 Experiment : human_TATA\n",
            "precision : 0.9357798165137615\n",
            "recall : 0.6986301369863014\n",
            "MCC : 0.6726432857191329\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 190 Experiment : human_TATA\n",
            "precision : 0.9237668161434978\n",
            "recall : 0.7054794520547946\n",
            "MCC : 0.6661250589709387\n",
            "Update best recall\n",
            "Epoch : 200 Experiment : human_TATA\n",
            "precision : 0.9484978540772532\n",
            "recall : 0.7568493150684932\n",
            "MCC : 0.7308273089915175\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 210 Experiment : human_TATA\n",
            "precision : 0.9218106995884774\n",
            "recall : 0.7671232876712328\n",
            "MCC : 0.7121533716308227\n",
            "Update best recall\n",
            "Epoch : 220 Experiment : human_TATA\n",
            "precision : 0.9227467811158798\n",
            "recall : 0.7363013698630136\n",
            "MCC : 0.6888659323987031\n",
            "Epoch : 230 Experiment : human_TATA\n",
            "precision : 0.9504132231404959\n",
            "recall : 0.7876712328767124\n",
            "MCC : 0.757767120966401\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 240 Experiment : human_TATA\n",
            "precision : 0.9322709163346613\n",
            "recall : 0.8013698630136986\n",
            "MCC : 0.7505865127891055\n",
            "Update best recall\n",
            "Epoch : 250 Experiment : human_TATA\n",
            "precision : 0.9551020408163265\n",
            "recall : 0.8013698630136986\n",
            "MCC : 0.7737879668947788\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 260 Experiment : human_TATA\n",
            "precision : 0.9182879377431906\n",
            "recall : 0.8082191780821918\n",
            "MCC : 0.7416483279103844\n",
            "Update best recall\n",
            "Epoch : 270 Experiment : human_TATA\n",
            "precision : 0.915057915057915\n",
            "recall : 0.8116438356164384\n",
            "MCC : 0.7410489519279744\n",
            "Update best recall\n",
            "Epoch : 280 Experiment : human_TATA\n",
            "precision : 0.9137254901960784\n",
            "recall : 0.797945205479452\n",
            "MCC : 0.7284746041029388\n",
            "Epoch : 290 Experiment : human_TATA\n",
            "precision : 0.9250936329588015\n",
            "recall : 0.8458904109589042\n",
            "MCC : 0.7802622493765766\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 300 Experiment : human_TATA\n",
            "precision : 0.956\n",
            "recall : 0.8184931506849316\n",
            "MCC : 0.7890265358348204\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 310 Experiment : human_TATA\n",
            "precision : 0.9360902255639098\n",
            "recall : 0.8527397260273972\n",
            "MCC : 0.7976890061348133\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 320 Experiment : human_TATA\n",
            "precision : 0.9360902255639098\n",
            "recall : 0.8527397260273972\n",
            "MCC : 0.7976890061348133\n",
            "Epoch : 330 Experiment : human_TATA\n",
            "precision : 0.9647058823529412\n",
            "recall : 0.8424657534246576\n",
            "MCC : 0.8182392472625427\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 340 Experiment : human_TATA\n",
            "precision : 0.937037037037037\n",
            "recall : 0.8664383561643836\n",
            "MCC : 0.8105229134658228\n",
            "Update best recall\n",
            "Epoch : 350 Experiment : human_TATA\n",
            "precision : 0.9465648854961832\n",
            "recall : 0.8493150684931506\n",
            "MCC : 0.8056330530626206\n",
            "Epoch : 360 Experiment : human_TATA\n",
            "precision : 0.9578544061302682\n",
            "recall : 0.8561643835616438\n",
            "MCC : 0.8231450788510758\n",
            "Update best MCC\n",
            "Epoch : 370 Experiment : human_TATA\n",
            "precision : 0.9433962264150944\n",
            "recall : 0.8561643835616438\n",
            "MCC : 0.8082572032653899\n",
            "Epoch : 380 Experiment : human_TATA\n",
            "precision : 0.9505703422053232\n",
            "recall : 0.8561643835616438\n",
            "MCC : 0.8156765099375703\n",
            "Epoch : 390 Experiment : human_TATA\n",
            "precision : 0.948905109489051\n",
            "recall : 0.8904109589041096\n",
            "MCC : 0.8440709989029801\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 400 Experiment : human_TATA\n",
            "precision : 0.929368029739777\n",
            "recall : 0.8561643835616438\n",
            "MCC : 0.793561451276253\n",
            "Epoch : 410 Experiment : human_TATA\n",
            "precision : 0.9333333333333333\n",
            "recall : 0.863013698630137\n",
            "MCC : 0.8036540752161124\n",
            "Epoch : 420 Experiment : human_TATA\n",
            "precision : 0.9548872180451128\n",
            "recall : 0.8698630136986302\n",
            "MCC : 0.832072152950969\n",
            "Epoch : 430 Experiment : human_TATA\n",
            "precision : 0.945054945054945\n",
            "recall : 0.8835616438356164\n",
            "MCC : 0.8339591068816351\n",
            "Epoch : 440 Experiment : human_TATA\n",
            "precision : 0.9543726235741445\n",
            "recall : 0.8595890410958904\n",
            "MCC : 0.8225598560129929\n",
            "Epoch : 450 Experiment : human_TATA\n",
            "precision : 0.935251798561151\n",
            "recall : 0.8904109589041096\n",
            "MCC : 0.8297213299089478\n",
            "Epoch : 460 Experiment : human_TATA\n",
            "precision : 0.969811320754717\n",
            "recall : 0.8801369863013698\n",
            "MCC : 0.8564086962258812\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 470 Experiment : human_TATA\n",
            "precision : 0.9363295880149812\n",
            "recall : 0.8561643835616438\n",
            "MCC : 0.8008859211662659\n",
            "Epoch : 480 Experiment : human_TATA\n",
            "precision : 0.9622641509433962\n",
            "recall : 0.8732876712328768\n",
            "MCC : 0.842651126808598\n",
            "Epoch : 490 Experiment : human_TATA\n",
            "precision : 0.9588014981273408\n",
            "recall : 0.8767123287671232\n",
            "MCC : 0.8421332647456444\n",
            "Epoch : 500 Experiment : human_TATA\n",
            "precision : 0.9659090909090909\n",
            "recall : 0.8732876712328768\n",
            "MCC : 0.8463658944408897\n",
            "Epoch : 510 Experiment : human_TATA\n",
            "precision : 0.9628252788104089\n",
            "recall : 0.886986301369863\n",
            "MCC : 0.8553974085185584\n",
            "Epoch : 520 Experiment : human_TATA\n",
            "precision : 0.9343065693430657\n",
            "recall : 0.8767123287671232\n",
            "MCC : 0.8166215355240214\n",
            "Epoch : 530 Experiment : human_TATA\n",
            "precision : 0.9518518518518518\n",
            "recall : 0.8801369863013698\n",
            "MCC : 0.8379982664646642\n",
            "Epoch : 540 Experiment : human_TATA\n",
            "precision : 0.948905109489051\n",
            "recall : 0.8904109589041096\n",
            "MCC : 0.8440709989029801\n",
            "Epoch : 550 Experiment : human_TATA\n",
            "precision : 0.9252669039145908\n",
            "recall : 0.8904109589041096\n",
            "MCC : 0.8190745402862489\n",
            "Epoch : 560 Experiment : human_TATA\n",
            "precision : 0.9592592592592593\n",
            "recall : 0.886986301369863\n",
            "MCC : 0.851735942964085\n",
            "precision : 0.9548872180451128\n",
            "recall : 0.8639455782312925\n",
            "MCC : 0.8268878523061103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-e5b8bbb196de>:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(best_model))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved epoch 560 as best path. using this for testing"
      ],
      "metadata": {
        "id": "6gqYXpBPU5G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "o4t8UJDGU96d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test('/content/drive/MyDrive/data/human/TATA/hs_pos_TATA.txt', pretrain = '/content/output/human_TATA/epoch_560.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a12GorbDRKbU",
        "outputId": "d10c1a1b-e318-4564-b607-0014f1efeb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-c10a607ddafe>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(pretrain))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  ...]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Human Non TATA"
      ],
      "metadata": {
        "id": "YI2CI-8vVv8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train('/content/drive/MyDrive/data/human/nonTATA/hs_pos_nonTATA.txt', exp_name=\"human_non_TATA\", training=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEnjw1wIVmKf",
        "outputId": "5a8693d9-d15b-4103-833f-9bbe68a360db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading\n",
            "Key error :  N 200\n",
            "Key error :  N 200\n",
            "Start training\n",
            "Epoch : 0 Experiment : human_non_TATA\n",
            "precision : 0.8718980549966465\n",
            "recall : 0.5017367811655732\n",
            "MCC : 0.4727382049857469\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 10 Experiment : human_non_TATA\n",
            "precision : 0.9102202145680407\n",
            "recall : 0.6221536086453107\n",
            "MCC : 0.5911742022606917\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 20 Experiment : human_non_TATA\n",
            "precision : 0.92253136933988\n",
            "recall : 0.6526437668853724\n",
            "MCC : 0.625190847473307\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 30 Experiment : human_non_TATA\n",
            "precision : 0.9396267837541163\n",
            "recall : 0.6607487456580471\n",
            "MCC : 0.6474685631153649\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 40 Experiment : human_non_TATA\n",
            "precision : 0.9551924090669478\n",
            "recall : 0.6993438826707835\n",
            "MCC : 0.6918166104549979\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 50 Experiment : human_non_TATA\n",
            "precision : 0.9711229946524064\n",
            "recall : 0.700887688151293\n",
            "MCC : 0.7080108884919442\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 60 Experiment : human_non_TATA\n",
            "precision : 0.9738298943130347\n",
            "recall : 0.7468159011964492\n",
            "MCC : 0.7473360894989421\n",
            "Update best precision\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 70 Experiment : human_non_TATA\n",
            "precision : 0.9782057780030411\n",
            "recall : 0.7448861443458125\n",
            "MCC : 0.7499347860259772\n",
            "Update best precision\n",
            "Update best MCC\n",
            "Epoch : 80 Experiment : human_non_TATA\n",
            "precision : 0.9692020531964536\n",
            "recall : 0.8016209957545349\n",
            "MCC : 0.7880170877302297\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 90 Experiment : human_non_TATA\n",
            "precision : 0.9740673339399454\n",
            "recall : 0.8263218834426862\n",
            "MCC : 0.813737745171082\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 100 Experiment : human_non_TATA\n",
            "precision : 0.9808946877912396\n",
            "recall : 0.8124276341181011\n",
            "MCC : 0.8086190164933847\n",
            "Update best precision\n",
            "Epoch : 110 Experiment : human_non_TATA\n",
            "precision : 0.9650597080937638\n",
            "recall : 0.8421458896179081\n",
            "MCC : 0.8183200893631917\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 120 Experiment : human_non_TATA\n",
            "precision : 0.9762118491921006\n",
            "recall : 0.8394442300270166\n",
            "MCC : 0.8271466988788873\n",
            "Update best MCC\n",
            "Epoch : 130 Experiment : human_non_TATA\n",
            "precision : 0.9691629955947136\n",
            "recall : 0.8490930142802007\n",
            "MCC : 0.8284589413221105\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 140 Experiment : human_non_TATA\n",
            "precision : 0.9760213143872114\n",
            "recall : 0.8483211115399459\n",
            "MCC : 0.8346545740915824\n",
            "Update best MCC\n",
            "Epoch : 150 Experiment : human_non_TATA\n",
            "precision : 0.9696581196581197\n",
            "recall : 0.8757236588189888\n",
            "MCC : 0.8523298974296636\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 160 Experiment : human_non_TATA\n",
            "precision : 0.9716404886561955\n",
            "recall : 0.8595137012736396\n",
            "MCC : 0.8400390238400732\n",
            "Epoch : 170 Experiment : human_non_TATA\n",
            "precision : 0.9760452961672473\n",
            "recall : 0.8649170204554226\n",
            "MCC : 0.8492118629847083\n",
            "Epoch : 180 Experiment : human_non_TATA\n",
            "precision : 0.9724612736660929\n",
            "recall : 0.8722500964878426\n",
            "MCC : 0.852085473988979\n",
            "Epoch : 190 Experiment : human_non_TATA\n",
            "precision : 0.9727311461440137\n",
            "recall : 0.8811269780007719\n",
            "MCC : 0.8602490977966856\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 200 Experiment : human_non_TATA\n",
            "precision : 0.972422571064913\n",
            "recall : 0.8846005403319182\n",
            "MCC : 0.8630405463302144\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 210 Experiment : human_non_TATA\n",
            "precision : 0.9702633814783348\n",
            "recall : 0.8815129293708993\n",
            "MCC : 0.8580936432353079\n",
            "Epoch : 220 Experiment : human_non_TATA\n",
            "precision : 0.9698000850701829\n",
            "recall : 0.8799691238903898\n",
            "MCC : 0.8562477960151949\n",
            "Epoch : 230 Experiment : human_non_TATA\n",
            "precision : 0.9727659574468085\n",
            "recall : 0.882284832111154\n",
            "MCC : 0.8613179517553943\n",
            "Epoch : 240 Experiment : human_non_TATA\n",
            "precision : 0.9704142011834319\n",
            "recall : 0.8861443458124276\n",
            "MCC : 0.8623855374617773\n",
            "Update best recall\n",
            "Epoch : 250 Experiment : human_non_TATA\n",
            "precision : 0.970787468247248\n",
            "recall : 0.8849864917020456\n",
            "MCC : 0.8617281537173627\n",
            "Epoch : 260 Experiment : human_non_TATA\n",
            "precision : 0.9697097181320993\n",
            "recall : 0.8896179081435739\n",
            "MCC : 0.8647841067122404\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 270 Experiment : human_non_TATA\n",
            "precision : 0.9769427839453458\n",
            "recall : 0.8830567348514087\n",
            "MCC : 0.866224680272974\n",
            "Update best MCC\n",
            "Epoch : 280 Experiment : human_non_TATA\n",
            "precision : 0.9605749486652977\n",
            "recall : 0.9027402547279043\n",
            "MCC : 0.8672622862846596\n",
            "Update best recall\n",
            "Update best MCC\n",
            "Epoch : 290 Experiment : human_non_TATA\n",
            "precision : 0.98\n",
            "recall : 0.8888460054033192\n",
            "MCC : 0.8744974343655129\n",
            "Update best MCC\n",
            "Epoch : 300 Experiment : human_non_TATA\n",
            "precision : 0.9739495798319328\n",
            "recall : 0.8946352759552296\n",
            "MCC : 0.8736078972296186\n",
            "Epoch : 310 Experiment : human_non_TATA\n",
            "precision : 0.9658014009064689\n",
            "recall : 0.9046700115785411\n",
            "MCC : 0.8743893767524366\n",
            "Update best recall\n",
            "Epoch : 320 Experiment : human_non_TATA\n",
            "precision : 0.9657731958762886\n",
            "recall : 0.9038981088382864\n",
            "MCC : 0.8736590427326446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ieTpMZZWWCE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}